{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJrRJLBS7DWn"
   },
   "source": [
    "# Softmax exercise\n",
    "\n",
    "We will implement a softmax classifier that is trained on the CIFAR10 dataset. The output is a model that is able to classify the input image into the 10 different classes of the CIFAR10 dataset.\n",
    "\n",
    "Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission.\n",
    "\n",
    "You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- use a validation set to **tune the learning rate** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KQ8b6HtX7DWs"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle\n",
    "from random import shuffle\n",
    "\n",
    "from six.moves import cPickle as pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from imageio import imread\n",
    "import platform\n",
    "from torchvision import datasets\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rb1FNWrPvqVB"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmv_iR7VumNQ"
   },
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vW0orz8Z7O0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "def load_pickle(f):\n",
    "    version = platform.python_version_tuple()\n",
    "    if version[0] == '2':\n",
    "        return  pickle.load(f)\n",
    "    elif version[0] == '3':\n",
    "        return  pickle.load(f, encoding='latin1')\n",
    "    raise ValueError(\"invalid python version: {}\".format(version))\n",
    "\n",
    "def load_CIFAR_batch(filename):\n",
    "  \"\"\" load single batch of cifar \"\"\"\n",
    "  with open(filename, 'rb') as f:\n",
    "    datadict = load_pickle(f)\n",
    "    X = datadict['data']\n",
    "    Y = datadict['labels']\n",
    "    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "    Y = np.array(Y)\n",
    "    return X, Y\n",
    "\n",
    "def load_CIFAR10(ROOT):\n",
    "  \"\"\" load all of cifar \"\"\"\n",
    "  xs = []\n",
    "  ys = []\n",
    "  datasets.CIFAR10(\n",
    "        root=\"data/\", train=True,\n",
    "        download=True, transform=None,\n",
    "    )\n",
    "  for b in range(1,6):\n",
    "    f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
    "    X, Y = load_CIFAR_batch(f)\n",
    "    xs.append(X)\n",
    "    ys.append(Y)    \n",
    "  Xtr = np.concatenate(xs)\n",
    "  Ytr = np.concatenate(ys)\n",
    "  del X, Y\n",
    "  return Xtr, Ytr\n",
    "\n",
    "    \n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'data/cifar-10-batches-py'\n",
    "    X_train, y_train = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    \n",
    "    # Normalize the data: subtract the mean image and divide by variance\n",
    "    mean = np.array([0.4914, 0.4822, 0.4465])\n",
    "    std = np.array([0.2023, 0.1994, 0.2010])\n",
    "    X_train = np.divide(np.subtract( X_train/255 , mean[np.newaxis,np.newaxis,:]), std[np.newaxis,np.newaxis,:])\n",
    "    X_val = np.divide(np.subtract( X_val/255 , mean[np.newaxis,np.newaxis,:]), std[np.newaxis,np.newaxis,:])\n",
    "\n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "\n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWxBSsLD7DWv"
   },
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Here, you implement **softmax_loss_vectorized**. This function just returns the loss and gradient after applying the softmax function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aFed4lnK7yfC"
   },
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "def softmax_loss_vectorized(W, X, y):\n",
    "    \"\"\"\n",
    "    Softmax loss function, vectorized version.\n",
    "\n",
    "    Inputs have dimension D, there are C classes, and we operate on minibatches\n",
    "    of N examples.\n",
    "\n",
    "    Inputs:\n",
    "    - W: A numpy array of shape (D, C) containing weights.\n",
    "    - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
    "    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
    "    that X[i] has label c, where 0 <= c < C.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss as single float\n",
    "    - gradient with respect to weights W; an array of same shape as W\n",
    "    \"\"\"\n",
    "    # Initialize the loss and gradient to zero.\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "\n",
    "    #############################################################################\n",
    "    # TODO: Compute the softmax loss and its gradient using no explicit loops.  #\n",
    "    # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
    "    # here, it is easy to run into numeric instability.                         #\n",
    "    #############################################################################\n",
    "\n",
    "    y_one_hot = np.eye(10)[y]\n",
    "    \n",
    "    fwd_pass = X.dot(W)\n",
    "    print(X.shape, W.shape, fwd_pass.shape)\n",
    "\n",
    "\n",
    "    prob = [np.exp(v) / np.sum(np.exp(v)) for v in fwd_pass]\n",
    "    prob = -np.log(loss)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    #############################################################################\n",
    "    #                          END OF YOUR CODE                                 #\n",
    "    #############################################################################\n",
    "\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtMJHoylvbEx"
   },
   "source": [
    "## This is the Softmax Linear Classifier.\n",
    "### Implement SGD in the train function.\n",
    "### Write the predict function to evaluate the performance on both the training and validation set\n",
    "### Do not touch the loss function. You already defined it above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nuhecHcsvJU7"
   },
   "outputs": [],
   "source": [
    "class LinearClassifier(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.W = None\n",
    "\n",
    "    def train(self, X, y, learning_rate=1e-3, num_iters=100,\n",
    "            batch_size=200, verbose=False):\n",
    "        \"\"\"\n",
    "        Train this linear classifier using stochastic gradient descent.\n",
    "\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
    "            training samples each of dimension D.\n",
    "        - y: A numpy array of shape (N,) containing training labels; y[i] = c\n",
    "            means that X[i] has label 0 <= c < C for C classes.\n",
    "        - learning_rate: (float) learning rate for optimization.\n",
    "        - num_iters: (integer) number of steps to take when optimizing\n",
    "        - batch_size: (integer) number of training examples to use at each step.\n",
    "        - verbose: (boolean) If true, print progress during optimization.\n",
    "\n",
    "        Outputs:\n",
    "        A list containing the value of the loss function at each training iteration.\n",
    "        \"\"\"\n",
    "        num_train, dim = X.shape\n",
    "        num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n",
    "        if self.W is None:\n",
    "            # lazily initialize W\n",
    "            self.W = 0.001 * np.random.randn(dim, num_classes)\n",
    "\n",
    "        # Run stochastic gradient descent to optimize W\n",
    "        loss_history = []\n",
    "        for it in range(num_iters):\n",
    "            #########################################################################\n",
    "            # TODO:                                                                 #\n",
    "            # Sample batch_size elements from the training data and their           #\n",
    "            # corresponding labels to use in this round of gradient descent.        #\n",
    "            # Store the data in X_batch and their corresponding labels in           #\n",
    "            # y_batch; after sampling X_batch should have shape (dim, batch_size)   #\n",
    "            # and y_batch should have shape (batch_size,)                           #\n",
    "            #                                                                       #\n",
    "            # Hint: Use np.random.choice to generate indices. Sampling with         #\n",
    "            # replacement is faster than sampling without replacement.              #\n",
    "            #########################################################################\n",
    "            random_mask = np.random.choice(num_train, batch_size)\n",
    "            X_batch = X[random_mask]\n",
    "            y_batch = y[random_mask]\n",
    "            #########################################################################\n",
    "            #                       END OF YOUR CODE                                #\n",
    "            #########################################################################\n",
    "\n",
    "            # evaluate loss and gradient\n",
    "            loss, grad = self.loss(X_batch, y_batch)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            # perform parameter update\n",
    "            #########################################################################\n",
    "            # TODO:                                                                 #\n",
    "            # Update the weights using the gradient and the learning rate.          #\n",
    "            #########################################################################\n",
    "            self.W -= learning_rate * grad\n",
    "            #########################################################################\n",
    "            #                       END OF YOUR CODE                                #\n",
    "            #########################################################################\n",
    "\n",
    "            if verbose and it % 100 == 0:\n",
    "                print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
    "\n",
    "        return loss_history\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained weights of this linear classifier to predict labels for\n",
    "        data points.\n",
    "\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
    "        training samples each of dimension D.\n",
    "\n",
    "        Returns:\n",
    "        - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
    "        array of length N, and each element is an integer giving the predicted\n",
    "        class.\n",
    "        \"\"\"\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "        ###########################################################################\n",
    "        # TODO:                                                                   #\n",
    "        # Implement this method. Store the predicted labels in y_pred.            #\n",
    "        ###########################################################################\n",
    "        y_pred = X.dot(self.W)\n",
    "        ###########################################################################\n",
    "        #                           END OF YOUR CODE                              #\n",
    "        ###########################################################################\n",
    "        return y_pred\n",
    "  \n",
    "    def loss(self, X_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Compute the loss function and its derivative. \n",
    "        Subclasses will override this.\n",
    "\n",
    "        Inputs:\n",
    "        - X_batch: A numpy array of shape (N, D) containing a minibatch of N\n",
    "            data points; each point has dimension D.\n",
    "        - y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n",
    "\n",
    "        Returns: A tuple containing:\n",
    "        - loss as a single float\n",
    "        - gradient with respect to self.W; an array of the same shape as W\n",
    "        \"\"\"\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dzy_rjabwTUY"
   },
   "source": [
    "Finally, the softmax class is inherited from LinearClassifier and uses the softmax_loss_vectorized function as its loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "GTqa5eihvOvz"
   },
   "outputs": [],
   "source": [
    "class Softmax(LinearClassifier):\n",
    "    \"\"\" A subclass that uses the Softmax + Cross-entropy loss function \"\"\"\n",
    "\n",
    "    def loss(self, X_batch, y_batch):\n",
    "        return softmax_loss_vectorized(self.W, X_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BomGJP517DWv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3073) (3073, 10) (1000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54649/903068839.py:36: RuntimeWarning: divide by zero encountered in log\n",
      "  prob = -np.log(loss)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not numpy.float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m W \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m3073\u001b[39m, \u001b[38;5;241m10\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.0001\u001b[39m\n\u001b[1;32m      2\u001b[0m tic \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 3\u001b[0m loss_vectorized, grad_vectorized \u001b[38;5;241m=\u001b[39m \u001b[43msoftmax_loss_vectorized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m toc \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvectorized loss: \u001b[39m\u001b[38;5;132;01m%e\u001b[39;00m\u001b[38;5;124m computed in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (loss_vectorized, toc \u001b[38;5;241m-\u001b[39m tic))\n",
      "Cell \u001b[0;32mIn[8], line 37\u001b[0m, in \u001b[0;36msoftmax_loss_vectorized\u001b[0;34m(W, X, y)\u001b[0m\n\u001b[1;32m     35\u001b[0m prob \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mexp(v) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mexp(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m fwd_pass]\n\u001b[1;32m     36\u001b[0m prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mlog(loss)\n\u001b[0;32m---> 37\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_one_hot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m#############################################################################\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#                          END OF YOUR CODE                                 #\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#############################################################################\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss, dW\n",
      "File \u001b[0;32m~/Documents/EPFL/MA4/DLAV/DLAV-2025/.venv/lib/python3.10/site-packages/torch/nn/functional.py:3494\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3493\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3495\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3501\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not numpy.float64"
     ]
    }
   ],
   "source": [
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_val, y_val)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6Pzyyf47DWw"
   },
   "outputs": [],
   "source": [
    "# Use the validation set to tune hyperparameters (learning rate). \n",
    "# You should experiment with different ranges for the learning\n",
    "# rates; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-7, 5e-4]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning.                                  #\n",
    "# Save the best trained softmax classifer in best_softmax.                     #\n",
    "################################################################################\n",
    "pass\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[lr]\n",
    "    print('lr %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vpj-_mvS7DWx"
   },
   "outputs": [],
   "source": [
    "# evaluate on val set\n",
    "# Evaluate the best softmax on val set\n",
    "y_val_pred = best_softmax.predict(X_val)\n",
    "val_accuracy = np.mean(y_val == y_val_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (val_accuracy, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CI4FL2_a7DWx"
   },
   "source": [
    "Save the best_softmax weights using pickle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4uK5bgB7DWy"
   },
   "outputs": [],
   "source": [
    "with open('drive/MyDrive/Colab Notebooks/softmax_weights.pkl', 'wb') as f:\n",
    "    pickle.dump(best_softmax.W, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrQpxEbY7DWz"
   },
   "source": [
    "Load the best_softmax weights using pickle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1VkRfIl7DWz"
   },
   "outputs": [],
   "source": [
    "with open('drive/MyDrive/Colab Notebooks/softmax_weights.pkl', 'rb') as f:\n",
    "    W = pickle.load(f)\n",
    "new_softmax = Softmax()\n",
    "new_softmax.W = W.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2W7b80F_xkWj"
   },
   "source": [
    "Below is the code to visualize the weights learned for different classes. Try to notice the interesting patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NTkVGxZS7DWz"
   },
   "outputs": [],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oYKmBFJb_Z2p"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Softmax.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
