{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Object Detection For Autonomous Driving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Objective\n",
        "\n",
        "The goal of this assignment is to fine-tune the YOLO object detection model on a subset of the KITTI dataset, widely used in autonomous driving research. The KITTI dataset provides annotated images for tasks like object detection, tracking, and segmentation, making it ideal for training models to detect objects such as cars, pedestrians, and cyclists.\n",
        "\n",
        "By completing this assignment, you will gain hands-on experience in dataset preparation, annotation conversion, model training, and performance evaluation using metrics like mAP (mean Average Precision). Additionally, you will explore the trade-offs between speed and accuracy, crucial for real-time applications like autonomous driving.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction: Object Detection\n",
        "\n",
        "Object detection is a fundamental task in computer vision that involves identifying and localizing objects within an image or video. \n",
        "\n",
        "The main techniques for object detection include traditional methods like sliding window and region-based approaches, as well as modern deep learning-based methods such as YOLO (You Only Look Once), SSD (Single Shot MultiBox Detector), and Faster R-CNN. These techniques leverage convolutional neural networks (CNNs) to extract features and predict bounding boxes and class labels for objects in real-time or near real-time. \n",
        "\n",
        "Widely used models for object detection include:\n",
        "- **YOLO (You Only Look Once)**: Known for its speed and efficiency in real-time applications.\n",
        "- **Faster R-CNN**: A two-stage detector that provides high accuracy but is slower compared to single-stage detectors.\n",
        "- **SSD (Single Shot MultiBox Detector)**: Balances speed and accuracy, making it suitable for various applications.\n",
        "- **RetinaNet**: Introduces the focal loss to address class imbalance in object detection.\n",
        "- **EfficientDet**: A family of models that optimize both accuracy and efficiency using compound scaling.\n",
        "\n",
        "In the context of autonomous vehicles, object detection plays a critical role in perceiving the environment, identifying obstacles, pedestrians, traffic signs, and other vehicles. This capability enables autonomous systems to make informed decisions, ensuring safety and efficiency in navigation and driving scenarios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vxpo_aaOWy8"
      },
      "source": [
        "## YOLO\n",
        "\n",
        "YOLO (You Only Look Once) is a state-of-the-art, real-time object detection system. Unlike traditional object detection methods that apply a classifier to different regions of an image, YOLO frames object detection as a single regression problem. It predicts bounding boxes and class probabilities directly from full images in one evaluation, making it extremely fast and efficient.\n",
        "\n",
        "### Model\n",
        "\n",
        "The YOLO model divides the input image into a grid, where each grid cell is responsible for predicting bounding boxes and their associated class probabilities. Key features of YOLO include:\n",
        "\n",
        "- **Unified Architecture**: YOLO uses a single convolutional neural network (CNN) to predict multiple bounding boxes and class probabilities simultaneously.\n",
        "- **Speed**: YOLO is optimized for real-time applications, capable of processing images at high frame rates.\n",
        "- **Global Context**: By considering the entire image during training and inference, YOLO reduces false positives and improves detection accuracy.\n",
        "- **Versions**: Over time, YOLO has evolved through multiple versions (e.g., YOLOv1 up to YOLOv11), each introducing architectural improvements, better loss functions, and enhanced performance.\n",
        "\n",
        "YOLO is widely used in applications such as autonomous driving, surveillance, and robotics, where real-time object detection is critical.\n",
        "\n",
        "For more details on YOLO, visit the [Ultralytics GitHub repository](https://github.com/ultralytics/ultralytics)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPbPqW7ZOWy-"
      },
      "source": [
        "## Theory Questions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpPCVavyOWy-"
      },
      "source": [
        "For the next questions, focus on the latest version of the model, YOLOv11"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm6Om1xTOWy-"
      },
      "source": [
        "Briefly describe the architecture used in YOLOv11 and the different losses used during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2NIHmhDOWy-"
      },
      "source": [
        "ANS:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dixUYXepOWy_"
      },
      "source": [
        "What are the metrics used to assess YOLO's performance for object detection?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40YmHrTLOWy_"
      },
      "source": [
        "ANS:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5o0q5hHOWy_"
      },
      "source": [
        "## Code\n",
        "\n",
        "In the next part of the assignment, you will load a YOLO model and fine-tune it on the provided subset of the KITTI dataset.\n",
        "You will then visualize the training losses and evaluate the model by running inference to assess its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XpFKooiOpPK",
        "outputId": "d932bfc9-ff50-4c66-c426-fbacaf1b8280"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics opencv-python-headless -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, cv2, glob, random, shutil, requests, zipfile\n",
        "from ultralytics import YOLO\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from IPython.display import Image, display\n",
        "import glob, \n",
        "\n",
        "import yaml\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91ZIV_VwDDF4",
        "outputId": "3f50ab30-ca31-4847-82ad-03829a299f1a"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------\n",
        "# STEP 1: Setup paths\n",
        "# ---------------------------------------------\n",
        "HOME = os.getcwd()\n",
        "DATASET_PATH = os.path.join(HOME, \"datasets\", \"kitti_subset\")\n",
        "IMAGES_PATH = os.path.join(DATASET_PATH, \"images\")\n",
        "LABELS_PATH = os.path.join(DATASET_PATH, \"labels\")\n",
        "\n",
        "TRAIN_IMG = os.path.join(IMAGES_PATH, \"train\")\n",
        "VAL_IMG = os.path.join(IMAGES_PATH, \"val\")\n",
        "TEST_IMG = os.path.join(IMAGES_PATH, \"test\")\n",
        "TRAIN_LABEL = os.path.join(LABELS_PATH, \"train\")\n",
        "VAL_LABEL = os.path.join(LABELS_PATH, \"val\")\n",
        "TEST_LABEL = os.path.join(LABELS_PATH, \"test\")\n",
        "\n",
        "for path in [TRAIN_IMG, VAL_IMG, TEST_IMG, TRAIN_LABEL, VAL_LABEL, TEST_LABEL]:\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "# ---------------------------------------------\n",
        "# STEP 2: Download and extract KITTI data\n",
        "# ---------------------------------------------\n",
        "KITTI_URL = \"https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_image_2.zip\"\n",
        "KITTI_LABELS_URL = \"https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_label_2.zip\"\n",
        "\n",
        "def download_and_extract(url, extract_to):\n",
        "    zip_path = os.path.join(HOME, url.split(\"/\")[-1])\n",
        "    if not os.path.exists(zip_path):\n",
        "        print(f\"Downloading {url}...\")\n",
        "        response = requests.get(url, stream=True)\n",
        "        with open(zip_path, \"wb\") as file:\n",
        "            for chunk in response.iter_content(chunk_size=1024):\n",
        "                if chunk:\n",
        "                    file.write(chunk)\n",
        "        print(\"Download complete.\")\n",
        "\n",
        "    print(f\"Extracting {zip_path}...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    print(\"Extraction complete.\")\n",
        "\n",
        "download_and_extract(KITTI_URL, IMAGES_PATH)\n",
        "download_and_extract(KITTI_LABELS_URL, LABELS_PATH)\n",
        "\n",
        "# ---------------------------------------------\n",
        "# STEP 3: Convert KITTI annotations to YOLO\n",
        "# ---------------------------------------------\n",
        "CLASS_MAP = {\"Car\": 0, \"Pedestrian\": 1, \"Cyclist\": 2}\n",
        "\n",
        "def convert_kitti_to_yolo(kitti_label_path, yolo_label_path, image_path):\n",
        "    with open(kitti_label_path, \"r\") as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        return\n",
        "    height, width, _ = img.shape\n",
        "\n",
        "    with open(yolo_label_path, \"w\") as yolo_file:\n",
        "        for line in lines:\n",
        "            elements = line.split()\n",
        "            class_name = elements[0]\n",
        "            if class_name not in CLASS_MAP:\n",
        "                continue  # Skip irrelevant classes\n",
        "\n",
        "            class_id = CLASS_MAP[class_name]\n",
        "            x1, y1, x2, y2 = map(float, elements[4:8])\n",
        "\n",
        "            # Convert to YOLO format\n",
        "            center_x = ((x1 + x2) / 2) / width\n",
        "            center_y = ((y1 + y2) / 2) / height\n",
        "            bbox_width = (x2 - x1) / width\n",
        "            bbox_height = (y2 - y1) / height\n",
        "\n",
        "            yolo_file.write(f\"{class_id} {center_x:.6f} {center_y:.6f} {bbox_width:.6f} {bbox_height:.6f}\\n\")\n",
        "\n",
        "# ---------------------------------------------\n",
        "# STEP 4: Prepare dataset (subset, split, convert)\n",
        "# ---------------------------------------------\n",
        "all_images = sorted(glob.glob(os.path.join(IMAGES_PATH, \"training\", \"image_2\", \"*.png\")))\n",
        "all_labels = sorted(glob.glob(os.path.join(LABELS_PATH, \"training\", \"label_2\", \"*.txt\")))\n",
        "\n",
        "paired_data = list(zip(all_images, all_labels))\n",
        "random.seed(42)\n",
        "random.shuffle(paired_data)\n",
        "\n",
        "# Keep small subset (e.g., 300 images total)\n",
        "subset_data = paired_data[:300]\n",
        "train_size = int(0.8 * len(subset_data))\n",
        "val_size = int(0.1 * len(subset_data))\n",
        "\n",
        "train_data = subset_data[:train_size]\n",
        "val_data = subset_data[train_size:train_size + val_size]\n",
        "test_data = subset_data[train_size + val_size:]\n",
        "\n",
        "def move_and_convert_files(data, img_dest, lbl_dest):\n",
        "    for img_path, lbl_path in data:\n",
        "        new_img_path = os.path.join(img_dest, os.path.basename(img_path))\n",
        "        new_lbl_path = os.path.join(lbl_dest, os.path.basename(lbl_path))\n",
        "        shutil.copy2(img_path, new_img_path)\n",
        "        convert_kitti_to_yolo(lbl_path, new_lbl_path, new_img_path)\n",
        "\n",
        "move_and_convert_files(train_data, TRAIN_IMG, TRAIN_LABEL)\n",
        "move_and_convert_files(val_data, VAL_IMG, VAL_LABEL)\n",
        "move_and_convert_files(test_data, TEST_IMG, TEST_LABEL)\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# STEP 5: Create data.yaml\n",
        "# ---------------------------------------------\n",
        "data_yaml = f\"\"\"\n",
        "path: {DATASET_PATH}\n",
        "train: images/train\n",
        "val: images/val\n",
        "test: images/test\n",
        "\n",
        "nc: 3\n",
        "names: ['Car', 'Pedestrian', 'Cyclist']\n",
        "\"\"\"\n",
        "\n",
        "with open(os.path.join(DATASET_PATH, \"data.yaml\"), \"w\") as f:\n",
        "    f.write(data_yaml)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbHADSr17hFN",
        "outputId": "757d97b9-6b7c-4868-cdb8-508e42650a37"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------\n",
        "# STEP 6: Fine-tune YOLOv11 model\n",
        "# ---------------------------------------------\n",
        "#TO-DO: Add the code to fine-tune the YOLOv5 model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3X8M5Hs8Dijg"
      },
      "source": [
        "# Inference\n",
        "\n",
        "After training, visualize the losses andmetrics calculated on the validation set during training.\n",
        "\n",
        "Additionally, make sure you visualize the confusion matrix to see per-class errors.\n",
        "\n",
        "For qualitative analysis, run inference on the model, visualise the bouding boxes and visually assess it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rcU2771UOWzB",
        "outputId": "6238380b-521b-46f4-bc44-d8cd11e72710"
      },
      "outputs": [],
      "source": [
        "# -------- VISUALIZE TRAINING LOSSES AND METRICS --------\n",
        "#TO-DO: Add the code to visualize the training losses and metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ------ VISUALIZE CONFUSION MATRIX --------\n",
        "#TO-DO: Add the code to visualize the confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jq3kytelU8Kx",
        "outputId": "c56a1d4d-99c9-4447-90e7-f078059e134b"
      },
      "outputs": [],
      "source": [
        "# -------- RUN INFERENCE AND PLOT SAMPLE OUTPUTS --------\n",
        "#TO-DO: Add the code to run inference and visualize the outputs results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Kxh4K_mEGPJ"
      },
      "source": [
        "## Question\n",
        "\n",
        "Comment on the performance of the model based on the visualizations you made and discuss the inference speed versus performance for YOLO on the driving datasets. Do you think we can use YOLO online on autonomous cars? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ans"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
